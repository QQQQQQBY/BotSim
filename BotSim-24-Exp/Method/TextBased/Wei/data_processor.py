
import os
import torch
import torch.nn as nn
from torch.autograd import Variable
import csv
import pandas as pd
import numpy as np
import sklearn
import json
glove_path = r"BotSim-24-Exp/Dataset/glove.6B.50d.txt"
torch.manual_seed(100)
class DataProcessor(object):
    def read_text(self,is_train_data):
        #Read raw text data
        datas = []
        labels = []

        if (is_train_data):
            json_filename = 'BotSim-24-Exp/Dataset/instru_train.json'
            with open(json_filename, 'r') as json_file:
                traindata = json.load(json_file)
            for line in traindata:
                if line['Response'] == "human":
                    datas.append(line['Input'])
                    labels.append([1, 0])  #Negative class label dimension [1,0]
                if line['Response'] == "human":
                    datas.append(line['Input'])
                    labels.append([0,1])


        else:
            json_filename = 'BotSim-24-Exp/Dataset/instru_test.json'
            with open(json_filename, 'r') as json_file:
                testdata = json.load(json_file)
            for line in testdata:
                if line['Response'] == "human":
                    datas.append(line['Input'])
                    labels.append([1, 0])  # Negative class label dimension [1,0]
                if line['Response'] == "human":
                    datas.append(line['Input'])
                    labels.append([0,1])


        return datas, labels

    def word_count(self, datas):
        #Count the frequency of words, and arrange them in descending order to find the most frequent words
        dic = {}
        for data in datas:
            data_list = data.split()
            for word in data_list:
                word = word.lower() #All words are converted to lower case
                if(word in dic):
                    dic[word] += 1
                else:
                    dic[word] = 1
        word_count_sorted = sorted(dic.items(), key=lambda item:item[1], reverse=True)
        return  word_count_sorted

    def word_index(self, datas, vocab_size):
        #Create a vocabulary
        word_count_sorted = self.word_count(datas)
        word2index = {}
        #Words that do not appear in the vocabulary list
        word2index["<unk>"] = 0
        #padding added to the sentence
        word2index["<pad>"] = 1

        #The actual size of the vocabulary is determined by the number of words and the qualified size
        vocab_size = min(len(word_count_sorted), vocab_size)
        for i in range(vocab_size):
            word = word_count_sorted[i][0]
            word2index[word] = i + 2

        return word2index, vocab_size

    def get_datasets(self, vocab_size, embedding_size, max_len):
        #Note, since the word embedding generated by nn.Embedding is not fixed each time, the word embedding of training data and the word embedding of test data are obtained here
        # Test data vocabularies are also created with training data
        train_datas, train_labels = self.read_text(is_train_data=True)

        word2index, vocab_size = self.word_index(train_datas, vocab_size)

        test_datas, test_labels = self.read_text(is_train_data = False)

        train_features = []
        ###glove
        embedding_index = dict()

        with open(glove_path, 'r', encoding='utf-8') as f:
            line = f.readline()
            while line:
                values = line.split()
                word = values[0]
                coefs = np.asarray(values[1:])
                embedding_index[word] = coefs
                line = f.readline()
        embedding_matrix = np.zeros((vocab_size + 2, embedding_size))
        for word, i in word2index.items():
            embedding_vector = embedding_index.get(word)
            if embedding_vector is not None:
                embedding_matrix[i] = embedding_vector


        for data in train_datas:
            feature = []
            data_list = data.split()
            for word in data_list:
                word = word.lower() #The words in the thesaurus are all lowercase
                if word in word2index:
                    feature.append(word2index[word])
                else:
                    feature.append(word2index["<unk>"]) #Words that do not appear in the vocabulary list are replaced with <unk>
                if(len(feature)==max_len): #Limit the maximum length of a sentence, and truncate any portion that exceeds it
                    break
            #padding is added to sentences that have not reached their maximum length
            feature = feature + [word2index["<pad>"]] * (max_len - len(feature))
            train_features.append(feature)

        test_features = []
        for data in test_datas:
            feature = []
            data_list = data.split()
            for word in data_list:
                word = word.lower() #The words in the thesaurus are all lowercase
                if word in word2index:
                    feature.append(word2index[word])
                else:
                    feature.append(word2index["<unk>"]) 
                if(len(feature)==max_len): 
                    break
            #padding is added to sentences that have not reached their maximum length
            feature = feature + [word2index["<pad>"]] * (max_len - len(feature))
            test_features.append(feature)

        #Convert the index of the word into tensor. The dimensions of the data in train_features must be consistent; otherwise, an error will be reported
        train_features = torch.LongTensor(train_features)
        train_labels = torch.FloatTensor(train_labels)

        test_features = torch.LongTensor(test_features)
        test_labels = torch.FloatTensor(test_labels)

        #Convert the word to embedding
        #There are two special words <unk> and <pad> in the thesaurus, so the actual size of the thesaurus is vocab_size + 2
        embed = nn.Embedding(vocab_size + 2, embedding_size)
        embed.weight = torch.nn.Parameter(torch.from_numpy(embedding_matrix))
        embed.weight.requires_grad = False
        train_features = embed(train_features)
        test_features = embed(test_features)

        # Change the word to float
        # train_features = torch.FloatTensor(train_features)
        # test_features = torch.FloatTensor(test_features)

        train_features = train_features.float()
        test_features = test_features.float()

        #Specifies whether the input feature needs to calculate the gradient
        train_features = Variable(train_features, requires_grad=False)
        train_datasets = torch.utils.data.TensorDataset(train_features, train_labels)

        test_features = Variable(test_features, requires_grad=False)
        test_datasets = torch.utils.data.TensorDataset(test_features, test_labels)
        return train_datasets, test_datasets